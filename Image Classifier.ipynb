{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42EXGRkhdvSi"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2h_5i8D2Ysd"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for training a neural network on the MNIST dataset\n",
    "\n",
    "# Learning rate controls the step size of gradient descent, determining how much to update the weights in each iteration.\n",
    "learning_rate = 0.02\n",
    "\n",
    "# Number of epochs specifies how many complete passes through the training dataset will be performed.\n",
    "num_epochs = 100\n",
    "\n",
    "# Batch size defines the number of images processed in each mini-batch during training.\n",
    "batch_size = 32\n",
    "\n",
    "# Percentage of the dataset to be used for training, allowing for experimentation with smaller subsets of data.\n",
    "train_data_percentage = 0.0025\n",
    "test_data_percentage = 0.1\n",
    "\n",
    "# Input size corresponds to the dimensions of the MNIST images, flattened from 28x28 pixels.\n",
    "input_size = 28 * 28  # MNIST image size\n",
    "\n",
    "# Number of output classes, representing the digits 0 through 9 in the MNIST dataset.\n",
    "num_classes = 10  # Number of classes in MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kf_wVsrPP1Je"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lfovmg2QPh-V"
   },
   "source": [
    "# Dataset characteristics\n",
    "We use the MNIST digit classification data set for this assignment.\n",
    "A total of 60K images for training and 10K images for testing are available. But we only use a small percentage of them. Images are 28 x 28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJlI-SUQnGd-"
   },
   "outputs": [],
   "source": [
    "# Load MNIST datasets, and create pytorch data loader to read data in mini-batches\n",
    "def get_data_loaders(train_data_percentage, test_data_percentage, batch_size, transform):\n",
    "    # Load the entire MNIST dataset\n",
    "    # For train and test data points we sometimes use different transforms.\n",
    "    # This becomes handy in the last task (data augmentation)\n",
    "    full_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    full_test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    # Calculate the size based on the percentage\n",
    "    train_size = int(train_data_percentage * len(full_train_dataset))  # percentage of training data\n",
    "    test_size = int(test_data_percentage * len(full_test_dataset))    # percentage of test data\n",
    "    train_remainder = len(full_train_dataset) - train_size\n",
    "    test_remainder = len(full_test_dataset) - test_size\n",
    "\n",
    "    # Split the dataset into the percentage specified and the remaining\n",
    "    train_dataset, _ = random_split(full_train_dataset, [train_size, train_remainder])\n",
    "    test_dataset, _ = random_split(full_test_dataset, [test_size, test_remainder])\n",
    "\n",
    "    # Create DataLoaders for batching and shuffling\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8msoGzhPi4N"
   },
   "outputs": [],
   "source": [
    "# Define transformation (convert to tensor)\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_loader, test_loader = get_data_loaders(train_data_percentage, test_data_percentage,batch_size, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J9VN0SrQ-wb"
   },
   "outputs": [],
   "source": [
    "# Get first batch of images and labels\n",
    "train_image_batch, classe_set = next(iter(train_loader))\n",
    "\n",
    "print(f'train_loader contains {len(train_loader)} batches of data.')\n",
    "print(f'train_image_batch has shape {train_image_batch.shape},')\n",
    "print('where 64 is the number of images in a batch, 1 is the number of image channels (1 for grayscale image),\\\n",
    " 28X28 stands for WxH (width and height of a single image).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xI_e2XvQg-u"
   },
   "source": [
    "# Visualization of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOaylU3kQi6I"
   },
   "outputs": [],
   "source": [
    "def show_gray_digits(image_set, row=2, col=3):\n",
    "    # Here we visualize some of the data points in the data set.\n",
    "    # Create a large figure, to be filled with multiple subplots.\n",
    "\n",
    "    # Since image_set is a tensor variable, we transform it to a numpy type variable.\n",
    "    image_set = image_set.cpu().detach().numpy()\n",
    "\n",
    "    for i in range(row*col):\n",
    "      # define subplot\n",
    "      plt.subplot(row, col, i+1)\n",
    "      # plot raw pixel data\n",
    "      plt.imshow(image_set[i,0], cmap=plt.get_cmap('gray'))\n",
    "    # show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCdEakslSrWJ"
   },
   "outputs": [],
   "source": [
    "# display images and their corresponding labels.\n",
    "show_gray_digits(train_image_batch, 2, 3)\n",
    "print(classe_set[:6])\n",
    "\n",
    "del train_image_batch, classe_set, train_loader, test_loader, transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asJiVvIi7DNi"
   },
   "source": [
    "# Linear SVM for MNIST classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06OPDpdTX87w"
   },
   "source": [
    "## Recap\n",
    "\n",
    "- **Accuracy Computation**: \n",
    "  - Calculated the accuracy on both the train and test sets after each epoch during training.\n",
    "  - Plotted the accuracies as a function of the number of epochs for both sets.\n",
    "\n",
    "- **Hinge Loss Computation**: \n",
    "  - Calculated the hinge loss on both the train and test sets after each epoch.\n",
    "  - Plotted the hinge loss values as a function of the number of epochs.\n",
    "\n",
    "- **Final Epoch Results**:\n",
    "  - Recorded the loss values and accuracies for the final epoch of training for both the train and test sets.\n",
    "\n",
    "- **Overfitting Analysis**:\n",
    "  - Analyzed whether the model exhibited significant overfitting, considering other potential factors contributing to the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V106p68mxF2f"
   },
   "outputs": [],
   "source": [
    "# Function to plot train/test loss and accuracy on separate subplots\n",
    "def plot_eval_results(train_loss, test_loss, train_acc, test_acc):\n",
    "    \"\"\"\n",
    "    Plots the training and testing loss/accuracy over the number of epochs.\n",
    "\n",
    "    Parameters:\n",
    "    - train_loss: list or array, the training loss values over epochs.\n",
    "    - test_loss: list or array, the testing loss values over epochs.\n",
    "    - train_acc: list or array, the training accuracy values over epochs.\n",
    "    - test_acc: list or array, the testing accuracy values over epochs.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Create subplots (1 row, 2 columns) for loss and accuracy\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))  # figsize sets the overall plot size\n",
    "\n",
    "    # Plot Loss on the first subplot\n",
    "    ax1.plot(range(len(train_loss)), train_loss, label=\"Training Loss\", color='blue')\n",
    "    ax1.plot(range(len(test_loss)), test_loss, label=\"Testing Loss\", color='red')\n",
    "\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(\"Train vs Test Loss over epochs\")\n",
    "\n",
    "\n",
    "    # Plot Accuracy on the second subplot\n",
    "    ax2.plot(range(len(train_acc)), train_acc, label=\"Training Accuracy\", color=\"blue\")\n",
    "    ax2.plot(range(len(test_acc)), test_acc, label=\"Testing Accuracy\", color=\"red\")\n",
    "\n",
    "\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title(\"Train vs Test Accuracy over epochs\")\n",
    "\n",
    "    # Adjust layout to avoid subplot overlap\n",
    "    plt.tight_layout()\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Display the plots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoFONXckubrE"
   },
   "outputs": [],
   "source": [
    "# Define a linear SVM model\n",
    "class LinearSVM(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LinearSVM, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the image\n",
    "        x= x.view(-1, input_size)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_X_hxJyuiyN"
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_loader, test_loader = get_data_loaders(train_data_percentage, test_data_percentage, batch_size, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UlGqcoeoRJX"
   },
   "outputs": [],
   "source": [
    "# Return model loss and accuracy with the provided criterion and data_loader.\n",
    "def eval(model, data_loader, criterion=None):\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  loss_batches = []\n",
    "\n",
    "  # Switch to evaluation mode and turn off gradient calculation\n",
    "  # since parameters are not updated during testing.\n",
    "  with torch.no_grad():\n",
    "      for images_batch, labels_batch in data_loader:\n",
    "          outputs = model(images_batch)\n",
    "          # The predicted label is the output with the highest activation.\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels_batch.size(0)\n",
    "          correct += (predicted == labels_batch).sum().item()\n",
    "\n",
    "          # Use provided criterion to calculate the loss for the mini batch\n",
    "          # Append the mini-batch loss to loss_batches array\n",
    "          batch_loss = criterion(outputs, labels_batch)\n",
    "          loss_batches.append(batch_loss.item())\n",
    "\n",
    "  accuracy = 100 * correct / total\n",
    "  loss = np.mean(loss_batches)\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5F9pY_zdpNs"
   },
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = LinearSVM(input_size, num_classes)\n",
    "criterion = nn.MultiMarginLoss()  # A Multi-class version of Hinge loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "train_loss_epochs = []\n",
    "test_loss_epochs = []\n",
    "train_accuracy_epochs = []\n",
    "test_accuracy_epochs = []\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images_batch, labels_batch) in enumerate(train_loader):\n",
    "        optimizer.zero_grad() # Clear the gradients\n",
    "        outputs = model(images_batch) # Forward pass\n",
    "        loss = criterion(outputs, labels_batch) # Calculate loss\n",
    "        loss.backward() # Backward pass\n",
    "        optimizer.step() # Update weights\n",
    "\n",
    "    # Obtain train/test loss values and accuracies after each epoch\n",
    "    train_accuracy, train_loss = eval(model, train_loader, criterion)\n",
    "    test_accuracy, test_loss = eval(model, test_loader, criterion)\n",
    "\n",
    "    print(f'Epoch {epoch+1:02d} - Train loss: {train_loss:.6f}, Train accuracy: {train_accuracy:.2f}%')\n",
    "    print(f'         - Test loss: {test_loss:.6f}, Test accuracy: {test_accuracy:.2f}%')\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "\n",
    "    train_loss_epochs.append(train_loss)\n",
    "    test_loss_epochs.append(test_loss)\n",
    "    train_accuracy_epochs.append(train_accuracy)\n",
    "    test_accuracy_epochs.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kxwxf94IvSLJ"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plot_eval_results(train_loss_epochs, test_loss_epochs, train_accuracy_epochs, test_accuracy_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4mZ3HLrmN1X"
   },
   "source": [
    "## Part C \n",
    "## Weight Decay Experiment\n",
    "\n",
    "- **Weight Decay Setup**:\n",
    "  - Set weight decay to values of **0.1**, **1**, and **10** when defining the SGD optimizer.\n",
    "  - Explored the impact of weight decay on regularization and model performance.\n",
    "\n",
    "- **Loss and Accuracy Computation**:\n",
    "  - Plotted the train and test losses and accuracies as a function of epochs for each weight decay setting.\n",
    "\n",
    "- **Final Epoch Results**:\n",
    "  - Recorded the loss and accuracy for both the train and test sets at the final epoch for each weight decay value.\n",
    "\n",
    "- **Analysis**:\n",
    "  - Analyzed whether weight decay improved the model's performance and whether it helped in reducing overfitting or underfitting. Justified the results based on the findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFRE4-dKmLkm"
   },
   "outputs": [],
   "source": [
    "for weight_decay in [0.1, 1, 10]:\n",
    "\n",
    "    linear_svm = LinearSVM(input_size, num_classes)\n",
    "    optimizer = torch.optim.SGD(linear_svm.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "\n",
    "    train_loss_epochs = []\n",
    "    test_loss_epochs = []\n",
    "    train_accuracy_epochs = []\n",
    "    test_accuracy_epochs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      for images_batch, labels_batch in train_loader:\n",
    "          optimizer.zero_grad() # Clear the gradients\n",
    "          outputs = linear_svm(images_batch) # Forward pass\n",
    "          loss = criterion(outputs, labels_batch) # Calculate loss\n",
    "          loss.backward() # Backward pass\n",
    "          optimizer.step() # Update weights\n",
    "\n",
    "\n",
    "      train_accuracy, train_loss = eval(linear_svm, train_loader, criterion)\n",
    "      test_accuracy, test_loss = eval(linear_svm, test_loader, criterion)\n",
    "\n",
    "      train_loss_epochs.append(train_loss)\n",
    "      test_loss_epochs.append(test_loss)\n",
    "\n",
    "      train_accuracy_epochs.append(train_accuracy)\n",
    "      test_accuracy_epochs.append(test_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "    # Plot accuracies\n",
    "    print(f'For weight decay value {weight_decay}:')\n",
    "    plot_eval_results(train_loss_epochs, test_loss_epochs, train_accuracy_epochs, test_accuracy_epochs)\n",
    "    print(f\"Final Train Loss: {train_loss}, Final Test Loss: {test_loss}\")\n",
    "    print(f\"Final Train Accuracy: {train_accuracy}, Final Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWxhX7sa7Sas"
   },
   "source": [
    "# Logistic Regression for MNIST classification.\n",
    "Here we implement a logistic regression model for the same MNIST classfication problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrHPqeNjYHbY"
   },
   "source": [
    "\n",
    "## Cross-Entropy Loss and Accuracy Computation\n",
    "\n",
    "- **Accuracy Computation**:\n",
    "  - Calculated the accuracy on both the train and test sets after each epoch during training.\n",
    "  - Plotted the accuracies for both sets as a function of the number of epochs.\n",
    "\n",
    "- **Cross-Entropy Loss Computation**:\n",
    "  - Calculated the cross-entropy loss on both the train and test sets after each epoch.\n",
    "  - Plotted the loss values for both sets as a function of the number of epochs.\n",
    "\n",
    "- **Final Epoch Results**:\n",
    "  - Recorded the loss values and accuracies for both the train and test sets at the final epoch of training.\n",
    "\n",
    "- **Overfitting Analysis**:\n",
    "  - Analyzed whether the model exhibited significant overfitting or if other factors (such as model architecture or training parameters) might explain any mediocre performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9zJ6UzNYI66"
   },
   "source": [
    "## Part C \n",
    "\n",
    "Comparing the results with SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nb4NBvIIzml6"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "      super(LogisticRegression, self).__init__()\n",
    "      self.fc = nn.Linear(input_size, num_classes, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the image\n",
    "        x= x.view(-1, input_size)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRaRLWiz7W9m"
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "logistic_model = LogisticRegression(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()  # A Multi-class version of Hinge loss\n",
    "optimizer = optim.SGD(logistic_model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_params = sum(p.numel() for p in logistic_model.parameters())\n",
    "print(f\"Number of parameters in the model: {total_params}\")\n",
    "\n",
    "\n",
    "# Train logistic regression model for MNIST classification\n",
    "\n",
    "train_loss_epochs = []\n",
    "test_loss_epochs = []\n",
    "train_accuracy_epochs = []\n",
    "test_accuracy_epochs = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for image, true_label_batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = logistic_model(image)\n",
    "    loss = criterion(outputs, true_label_batch)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  train_acc, train_loss = eval(logistic_model, train_loader, criterion)\n",
    "  test_acc, test_loss = eval(logistic_model, test_loader, criterion)\n",
    "\n",
    "\n",
    "  print(f'Epoch {epoch+1:02d} - Train loss: {train_loss:.6f}, Train accuracy: {train_acc:.2f}%')\n",
    "  print(f'         - Test loss: {test_loss:.6f}, Test accuracy: {test_acc:.2f}%')\n",
    "  print(\"-------------------------------------------------------------\")\n",
    "\n",
    "  train_loss_epochs.append(train_loss)\n",
    "  test_loss_epochs.append(test_loss)\n",
    "\n",
    "  train_accuracy_epochs.append(train_acc)\n",
    "  test_accuracy_epochs.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpTHhIHx2IvR"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFY-6zhb0Uet"
   },
   "outputs": [],
   "source": [
    "# Plot the loss values and accuracies for train/test\n",
    "plot_eval_results(train_loss_epochs, test_loss_epochs, train_accuracy_epochs, test_accuracy_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rW6Hed9_7cTl"
   },
   "source": [
    "#  Non-linearity \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxn4V824YZ9U"
   },
   "source": [
    "## Adding Hidden Layer and ReLU Activation\n",
    "\n",
    "- **Model Modifications**:\n",
    "  - Added a hidden layer with **5000 neurons** and a **ReLU activation** to both the **Logistic Regression** and **SVM** models.\n",
    "  - This was done for both Task 1 (accuracy computation) and Task 2 (loss computation).\n",
    "\n",
    "- **Train and Test Loss**:\n",
    "  - Plotted the **train loss** and **test loss** for both models (Logistic Regression and SVM) after each epoch.\n",
    "\n",
    "- **Train and Test Accuracies**:\n",
    "  - Plotted the **train accuracies** and **test accuracies** for both models (Logistic Regression and SVM) as a function of the number of epochs.\n",
    "\n",
    "- **Final Epoch Results**:\n",
    "  - Reported the **loss** and **accuracy** for both the **train** and **test** sets at the final epoch for both models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZbsCwxUYbIY"
   },
   "source": [
    "## Part B\n",
    "\n",
    "## Comparison of Results: Modified vs. Linear Models\n",
    "\n",
    "- **Model Comparison**:\n",
    "  - Compared the performance of the **modified models** (with a hidden layer of 5000 neurons and a ReLU activation) against the **linear models** (without weight decay).\n",
    "  - The comparison was based on metrics like **accuracy**, **loss**, and **overfitting** tendencies.\n",
    "\n",
    "- **Performance Analysis**:\n",
    "  - Evaluated which approach (linear model vs. modified model) produced **better results** in terms of accuracy and loss on both the **train** and **test** sets.\n",
    "  - Plotted the corresponding **train and test losses** and **accuracies** for both approaches.\n",
    "\n",
    "- **Overfitting Analysis**:\n",
    "  - Analyzed the tendency of each model to **overfit** based on the performance gap between the train and test sets.\n",
    "  - Discussed which model is **more prone to overfitting**, taking into account factors such as model complexity (with hidden layers vs. linear) and the absence of weight decay in the linear model.\n",
    "\n",
    "- **Findings**:\n",
    "  - Justified which approach works better based on the observed metrics.\n",
    "  - Provided an explanation for the **overfitting behavior** of each model, with insights into why the modified model (with the hidden layer and ReLU) may be more prone to overfitting due to its increased complexity compared to the simpler linear model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeaGbUHs7hM0"
   },
   "outputs": [],
   "source": [
    "# Both the SVM and Logistic Regression models we have in Task 1 and 2\n",
    "# can be changed to the ModifiedModel below.\n",
    "\n",
    "# Modified model with added neurons and relu layer\n",
    "class ModifiedModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "      super(ModifiedModel, self).__init__()\n",
    "      self.hidden_size = 5000\n",
    "      self.fc1 = nn.Linear(input_size, self.hidden_size) # input maps to 5000 neurons\n",
    "      self.fc2 = nn.Linear(self.hidden_size, num_classes) # 5000 mapped to our 10 digit for classification\n",
    "      self.relu = nn.ReLU() # activation function\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the image\n",
    "        x = x.view(-1, input_size)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAACCDvKjifY"
   },
   "outputs": [],
   "source": [
    "# Run the ModifiedModel with Hinge Loss (SVM)\n",
    "# Model initialization\n",
    "\n",
    "modified_SVM = ModifiedModel(input_size, num_classes)\n",
    "criterion = nn.MultiMarginLoss()\n",
    "optimizer = optim.SGD(modified_SVM.parameters(), lr= learning_rate)\n",
    "\n",
    "train_loss_epochs = []\n",
    "test_loss_epochs = []\n",
    "train_accuracy_epochs = []\n",
    "test_accuracy_epochs = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for image, true_label_batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = modified_SVM(image)\n",
    "    loss = criterion(outputs, true_label_batch)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  train_acc, train_loss = eval(modified_SVM, train_loader, criterion)\n",
    "  test_acc, test_loss = eval(modified_SVM, test_loader, criterion)\n",
    "\n",
    "\n",
    "  print(f'Epoch {epoch+1:02d} - Train loss: {train_loss:.6f}, Train accuracy: {train_acc:.2f}%')\n",
    "  print(f'         - Test loss: {test_loss:.6f}, Test accuracy: {test_acc:.2f}%')\n",
    "  print(\"-------------------------------------------------------------\")\n",
    "\n",
    "  train_loss_epochs.append(train_loss)\n",
    "  test_loss_epochs.append(test_loss)\n",
    "\n",
    "  train_accuracy_epochs.append(train_acc)\n",
    "  test_accuracy_epochs.append(test_acc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04rnHg1kjCsD"
   },
   "outputs": [],
   "source": [
    "# Plot the loss values and accuracies for train/test\n",
    "plot_eval_results(train_loss_epochs, test_loss_epochs, train_accuracy_epochs, test_accuracy_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7yU8_zZojZGG"
   },
   "outputs": [],
   "source": [
    "# Run the ModifiedModel with Cross Entropy Loss (Logistic Regression)\n",
    "# Model initialization\n",
    "\n",
    "modified_logistic_regression_model = ModifiedModel(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(modified_logistic_regression_model.parameters(), lr= learning_rate)\n",
    "\n",
    "train_loss_epochs = []\n",
    "test_loss_epochs = []\n",
    "train_accuracy_epochs = []\n",
    "test_accuracy_epochs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for image, true_label_batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = modified_logistic_regression_model(image)\n",
    "    loss = criterion(outputs, true_label_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  train_acc, train_loss = eval(modified_logistic_regression_model, train_loader, criterion)\n",
    "  test_acc, test_loss = eval(modified_logistic_regression_model, test_loader, criterion)\n",
    "\n",
    "\n",
    "  print(f'Epoch {epoch+1:02d} - Train loss: {train_loss:.6f}, Train accuracy: {train_acc:.2f}%')\n",
    "  print(f'         - Test loss: {test_loss:.6f}, Test accuracy: {test_acc:.2f}%')\n",
    "  print(\"-------------------------------------------------------------\")\n",
    "\n",
    "  train_loss_epochs.append(train_loss)\n",
    "  test_loss_epochs.append(test_loss)\n",
    "\n",
    "  train_accuracy_epochs.append(train_acc)\n",
    "  test_accuracy_epochs.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1E6gU-VbjHcO"
   },
   "outputs": [],
   "source": [
    "# Plot the loss values and accuracies for train/test\n",
    "plot_eval_results(train_loss_epochs, test_loss_epochs, train_accuracy_epochs, test_accuracy_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q09fVj0IB7LZ"
   },
   "source": [
    "# Data Augmentation \n",
    "\n",
    "## Data Augmentation Experiment\n",
    "\n",
    "### Augmentation Approaches Tested\n",
    "\n",
    "To improve the model’s performance, several alternative augmentation techniques were explored using PyTorch’s **transforms** module. The methods tested included:\n",
    "\n",
    "- **Random Rotation**: Rotating images by a random angle to help the model learn rotational invariance.\n",
    "- **Random Horizontal Flip**: Flipping images horizontally to simulate different viewing angles and increase dataset variability.\n",
    "- **Random Crop**: Applying random crops to images to help the model focus on different parts of the image and improve its ability to generalize.\n",
    "- **Color Jitter**: Adjusting the brightness, contrast, saturation, and hue of the images to simulate various lighting conditions.\n",
    "\n",
    "After applying the new augmentation methods, the model was retrained, and the **train and test accuracies** were plotted for each epoch. The results were compared with the baseline test accuracy from Task 3.\n",
    "\n",
    "### Results and Analysis\n",
    "\n",
    "- The final test accuracy, train/test accuracy plots, and loss values were recorded and compared across epochs.\n",
    "- The data augmentation technique that resulted in the **best improvement** (greater than 1% increase in test accuracy compared to Task 3) was identified.\n",
    "- The reason for the selected augmentation technique’s effectiveness was explained, demonstrating how it contributed to better model generalization.\n",
    "\n",
    "Overall, this task showed how the use of data augmentation helped improve the model's robustness, allowing it to handle variations in the input images and generalize better on the test set.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpyJUjzCqguo"
   },
   "outputs": [],
   "source": [
    "# TRANSFOMATION\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "train_loader_2, test_loader_2 = get_data_loaders(train_data_percentage, test_data_percentage, batch_size, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpZsz9sjMTfv"
   },
   "outputs": [],
   "source": [
    "# Run the ModifiedModel with Hinge Loss (SVM)\n",
    "# Model initialization\n",
    "\n",
    "model = ModifiedModel(input_size, num_classes)\n",
    "criterion = nn.MultiMarginLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "train_loss_epochs = []\n",
    "test_loss_epochs = []\n",
    "train_accuracy_epochs = []\n",
    "test_accuracy_epochs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for image, true_label_batch in train_loader_2:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(image)\n",
    "    loss = criterion(outputs, true_label_batch)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  train_acc, train_loss = eval(model, train_loader_2, criterion)\n",
    "  test_acc, test_loss = eval(model, test_loader_2, criterion)\n",
    "\n",
    "  train_loss_epochs.append(train_loss)\n",
    "  test_loss_epochs.append(test_loss)\n",
    "\n",
    "  train_accuracy_epochs.append(train_acc)\n",
    "  test_accuracy_epochs.append(test_acc)\n",
    "\n",
    "  print(f'Epoch {epoch+1:02d} - Train loss: {train_loss:.6f}, Train accuracy: {train_acc:.2f}%')\n",
    "  print(f'         - Test loss: {test_loss:.6f}, Test accuracy: {test_acc:.2f}%')\n",
    "  print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNOYq6y8jLVI"
   },
   "outputs": [],
   "source": [
    "# Plot the loss values and accuracies for train/test\n",
    "plot_eval_results(train_loss_epochs, test_loss_epochs, train_accuracy_epochs, test_accuracy_epochs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
